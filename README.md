# HDMapNet

**HDMapNet: An Online HD Map Construction and Evaluation Framework**

**ICRA 2022, CVPR 2021 Workshop best paper nominee**

[Qi Li](https://liqi17thu.github.io/), [Yue Wang](https://people.csail.mit.edu/yuewang/), [Yilun Wang](https://scholar.google.com.hk/citations?user=nUyTDosAAAAJ&hl=en/), [Hang Zhao](http://people.csail.mit.edu/hangzhao/)

**[[Paper](https://arxiv.org/abs/2107.06307)] [[Project Page](https://tsinghua-mars-lab.github.io/HDMapNet/)] [[5-min video](https://www.youtube.com/watch?v=AJ-rToTN8y8)]**

**Abstract:**
Estimating local semantics from sensory inputs is a central component for high-definition map constructions in autonomous driving. However, traditional pipelines require a vast amount of human efforts and resources in annotating and maintaining the semantics in the map, which limits its scalability. In this paper, we introduce the problem of local semantic map learning, which dynamically constructs the vectorized semantics based on onboard sensor observations. Meanwhile, we introduce a local semantic map learning method, dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras and/or point clouds from LiDAR, and predicts vectorized map elements in the bird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all settings, it performs better than baseline methods. Of note, our fusion-based HDMapNet outperforms existing methods by more than 50% in all metrics. In addition, we develop semantic-level and instance-level metrics to evaluate the map learning performance. Finally, we showcase our method is capable of predicting a locally consistent map. By introducing the method and metrics, we invite the community to study this novel map learning problem. Code and evaluation kit will be released to facilitate future development.

**Questions/Requests:** 
Please file an [issue](https://github.com/Tsinghua-MARS-Lab/HDMapNet-dev/issues) or email liqi17thu@gmail.com.

### Set up an virtual environment and install the repo.
```
sudo apt update
sudo apt list python3.*

sudo apt install python3.9 python3.9-venv

python3.9 -m venv .venv
. .venv/bin/activate

pip install torch==2.1.0+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html

pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html

pip install torchvision==0.16.0+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html

pip install -r requirement.txt
```

### Preparation
1. Download [nuScenes dataset](https://www.nuscenes.org/download) (the `Full dataset (v1.0)` part) and put it to your dataset folder.

2. Also download the `Map expansion` part and put it under the `maps` folder of your dataset.

### Label
Run `python vis_label.py /path/to/your/dataset/` for demo of vectorized labels. The visualizations are in `/your/dataset/nuScenes/samples/GT`.

### Training
Run `python train.py --instance_seg --direction_pred --version [v1.0-trainval, v1.0-mini] --logdir [output place]`. 

For example `python train.py --dataroot /home/ubuntu/study_dataset/nuScene/v1.0-trainval --instance_seg --direction_pred --version v1.0-trainval --logdir /home/ubuntu/ml_logs/hd_map_net/nuScene/v1.0-trainval/cam_lidar --model HDMapNet_fusion --nepochs 10`

### Visualize the prediction results
Run `python vis_pred.py --logdir /your/logdir/ --dataroot /your/dataroot/ --modelf /your/path/to/trained/model --instance_seg --direction_pred`

For example `python vis_pred.py --logdir ~/ml_logs/hd_map_net/nuScene/v1.0-trainval/cam_only/ --dataroot /home/ubuntu/study_dataset/nuScene/v1.0-trainval/ --modelf /home/ubuntu/ml_logs/hd_map_net/nuScene/v1.0-trainval/cam_only/model29.pt --instance_seg --direction_pred --version v1.0-trainval`

### Evaluation
Before running the evaluation code, you should get the `submission.json` file first, which can be generated by the following command.
```
python export_gt_to_json.py
```

Run `python evaluate.py --result_path [submission file path]` for evaluation. The script accepts vectorized or rasterized maps as input. For vectorized map, We firstly rasterize the vectors to map to do evaluation. For rasterized map, you should make sure the line width=1.

Below is the format for vectorized submission:

```
vectorized_submission {
    "meta": {
        "use_camera":   <bool>  -- Whether this submission uses camera data as an input.
        "use_lidar":    <bool>  -- Whether this submission uses lidar data as an input.
        "use_radar":    <bool>  -- Whether this submission uses radar data as an input.
        "use_external": <bool>  -- Whether this submission uses external data as an input.
        "vector":        true   -- Whether this submission uses vector format.
    },
    "results": {
        sample_token <str>: List[vectorized_line]  -- Maps each sample_token to a list of vectorized lines.
    }
}

vectorized_line {
    "pts":               List[<float, 2>]  -- Ordered points to define the vectorized line.
    "pts_num":           <int>,            -- Number of points in this line.
    "type":              <0, 1, 2>         -- Type of the line: 0: ped; 1: divider; 2: boundary
    "confidence_level":  <float>           -- Confidence level for prediction (used by Average Precision)
}
```

For rasterized submission, the format is:

```
rasterized_submisson {
    "meta": {
        "use_camera":   <bool>  -- Whether this submission uses camera data as an input.
        "use_lidar":    <bool>  -- Whether this submission uses lidar data as an input.
        "use_radar":    <bool>  -- Whether this submission uses radar data as an input.
        "use_external": <bool>  -- Whether this submission uses external data as an input.
        "vector":       false   -- Whether this submission uses vector format.
    },
    "results": {
        sample_token <str>: {  -- Maps each sample_token to a list of vectorized lines.
            "map": [<float, (C, H, W)>],         -- Raster map of prediction (C=0: ped; 1: divider 2: boundary). The value indicates the line idx (start from 1).
    	    "confidence_level": Array[float],    -- confidence_level[i] stands for confidence level for i^th line (start from 1). 
        }
    }
}
```

Run `python export_gt_to_json.py` to get a demo of vectorized submission. Run `python export_gt_to_json.py --raster` for rasterized submission.

Run `python export_pred_to_json.py --modelf [checkpoint]` to get submission file for trained model.

### Citation
If you found this paper or codebase useful, please cite our paper:
```
@misc{li2021hdmapnet,
      title={HDMapNet: An Online HD Map Construction and Evaluation Framework}, 
      author={Qi Li and Yue Wang and Yilun Wang and Hang Zhao},
      year={2021},
      eprint={2107.06307},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
